---
title: "Yes, AI Sucks. That Makes It The Best Time to Learn"
date: "December 18, 2025"
tags: ["AI", "Thoughts", "Innovation", "Learning"]
excerpt: "Today’s models fail loudly and visibly. That makes their limits easy to study and their mistakes easy to catch. Future systems will not be so forgiving."
readingTime: "4 min read"
---

There is a genre of AI writing that I see fairly regularly on LinkedIn, facebook, etc. etc. It usually starts by telling you your AI sounds smart. Then it tells you it isn't. It warns that models fabricate, lack judgment, and should never be trusted. The conclusion is always the same. Don't confuse coherence with intelligence.

Most of the claims in these posts are overstated, overwrought or just plain wrong. But those matter less than the posture they encourage.

The posture is fear dressed up as sophistication.

People are right to be uneasy about AI. Large language models sound confident and get things wrong. Treating them like analysts or decision makers is a category error. Delegating judgment to a system that has no concept of truth is irresponsible.

All of that is correct.

What is missing is the second half of the thought.

These are the worst models most of us will ever work with.

They are slow. They hallucinate. They require hand-holding. They fail loudly and often. That is exactly why this moment matters.

Right now, the cost of misuse is low. The failures are obvious. The stakes are mostly optional. You can choose where and how to apply these tools. You can experiment without wiring them directly into core decisions. You can see their limitations in plain view.

That won't last.

<blockquote class="pullquote">
Before someone builds a “super-bad robot,” someone has to build a “mildly bad robot,” and before that a “not-so-bad robot.” – Michio Kaku
</blockquote>

Future systems will be more capable and more convincing. They will fail less often and more quietly. They will be embedded deeper into workflows before most organizations understand how they behave. Learning how to work with probabilistic systems after you depend on them is much harder than learning before.

The real opportunity isn't in pretending these models reason or think. It is in learning how to design around their weaknesses.

That learning doesn't happen by standing at a distance and pointing out flaws. It happens by use.

Using these systems well means specific things.

It means separating generation from validation.
It means never treating an output as an answer.
It means forcing assumptions into the open.
It means building checks instead of trust.
It means deciding, explicitly, where judgment lives.

Whether a model “reasons” is a distraction. The only question that matters is whether you reason about its output.

Used this way, language models aren't analysts. They aren't strategists. They aren't authorities. They are tools for synthesis, exploration, and pressure-testing ideas. Sometimes they are noise. Sometimes they are useful. Your job is to know which is which.

Pundits who dismiss these systems outright often sound savvy. In practice, it is usually a performative move to signal distance from hype. It doesn't engage with the substance. And it misses the point.

The risk isn't that people will overestimate today’s models. The risk is that they will fail to build the discipline required for tomorrow.

The window where you can learn cheaply, visibly, and with low consequence is now. Ignoring it doesn't make you prudent. It makes you unprepared.


